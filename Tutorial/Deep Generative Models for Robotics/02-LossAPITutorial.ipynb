{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In deep generative models, model design means defining objective functions\n",
    "- Any deep generative models explicitly set the objective function to optimize\n",
    "    - Autoregressive modelsãƒ»Flow models: Kullback-Leibler divergence(log likelihood)\n",
    "    - VAE: Evidence lower bound\n",
    "    - GAN: Jensen-Shannon divergence(GAN also needs update of objective function itsself(=adversarial learning))\n",
    "- Regularization terms of inference or random variable representation is incorporated in the objective function\n",
    "<img src='../tutorial_figs/vae_loss_EN.png'>\n",
    "   \n",
    "    - In deep generative models, model design means defining objective functions\n",
    "    - Unlike traditional generative models, deep generative models don't inference by sampling\n",
    "- A framework that receives probability distributions and defines the objective functions\n",
    "    - LossAPI  \n",
    "<img src='../tutorial_figs/pixyz_API.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive probability distribution and define the objective function\n",
    "- Loss API document: https://pixyz.readthedocs.io/en/latest/losses.html#\n",
    "\n",
    "We take probability distributions defined in Distribution API and define the objective function.  \n",
    "In order to define the objective function, it needs these elements.  \n",
    "1. Calculate likelihood\n",
    "1. Calculate the distance between probability distribution\n",
    "1. Calculate the expected value\n",
    "1. Calculation considering data distribution(mean, sum)  \n",
    "\n",
    "In VAE loss, each elements corresponds as follows\n",
    "<img src='../tutorial_figs/vae_loss_API.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the loss\n",
    "Loss API needs input variable(`input_var`). The value of loss is calculated not until the input variable feeds into the loss.\n",
    "```python\n",
    "p = DistributionAPI()\n",
    "# define the objective function receiving distribution\n",
    "loss = LossAPI(p)\n",
    "# the value of loss is calculated when input_var is feeded\n",
    "loss_value = loss.eval({'input_var': input_data})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f35a0113c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixyz module\n",
    "from pixyz.distributions import Normal\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate likelihood\n",
    "When the observation $x_1$, ...., $x_N$ is obtained, we calculate the likelihood of the probability distribution p, which we assume x follows.  \n",
    "Here, we assume x follows a Gaussian distribution with mean=0, variance = 1.  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define probability distribution p\n",
    "x_dim = 5\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# observe x\n",
    "observed_x_num = 100\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log likelihood is calculated as follows:  \n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$  \n",
    "We can calculate log likelihood easily by using `LogProb()`.  \n",
    "To define log likelihood, We set the probability distribution defined in Pixyz distribution to `LogProb()`'s argument.  \n",
    "The value is calculated when observed data feeded into `LogProb.eval()`.  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# set the probability distribution to LogProb()'s arg\n",
    "log_likelihood_x = LogProb(p_nor_x)\n",
    "print_latex(log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -7.5539,  -6.8545,  -6.4024,  -5.8851,  -6.1517,  -8.3702,  -6.7028,\n",
      "         -5.0395,  -7.4346,  -7.1497,  -5.7594,  -7.3006, -11.9857,  -5.8238,\n",
      "         -6.7561,  -5.7640,  -6.2382,  -4.9060,  -6.1076,  -8.2535,  -7.8250,\n",
      "         -7.1956,  -7.6949,  -5.2324, -11.5860,  -8.1068,  -7.1763,  -8.3332,\n",
      "        -11.4631,  -6.6297,  -6.1200, -12.2358,  -5.3402,  -7.1465,  -7.5106,\n",
      "         -7.0829,  -6.6300,  -6.1832,  -7.2049, -10.8676,  -6.8674,  -5.8339,\n",
      "         -9.1939,  -7.5965,  -8.7743,  -7.3492,  -5.2578, -10.3097,  -6.5646,\n",
      "         -4.8807,  -5.9738,  -6.2394, -10.3945,  -9.1760,  -9.2957,  -5.5627,\n",
      "         -7.1047,  -6.4066,  -6.8100,  -6.0878,  -6.8835,  -7.9132,  -5.0738,\n",
      "         -8.8378,  -6.2286,  -5.8401,  -5.9691,  -5.6857,  -7.6903,  -6.4982,\n",
      "         -7.1259,  -8.7953, -10.5572,  -5.9161,  -7.0649,  -6.1292,  -6.0871,\n",
      "         -7.2513,  -7.2517,  -7.1378,  -6.4228,  -5.5728,  -5.6155,  -5.1962,\n",
      "         -8.3940,  -7.8178,  -9.8129,  -6.1119,  -5.0492,  -8.9898,  -6.9675,\n",
      "         -8.0218, -13.9816,  -6.8575,  -5.1304,  -5.5069,  -5.0561,  -5.1264,\n",
      "         -4.8489,  -5.4876])\n",
      "observed_x_num:  100\n"
     ]
    }
   ],
   "source": [
    "# The likelihood for each observation is calculated\n",
    "print(log_likelihood_x.eval({'x': observed_x}))\n",
    "# observed_x_num = 100\n",
    "print('observed_x_num: ', len(log_likelihood_x.eval({'x': observed_x})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`log_likelihood_x.eval({'x': observed_x})`'s output has the calculated result of  \n",
    "$\\log p(x_{1})$, $\\log p(x_{2})$, ...., $\\log p(x_{100})$  \n",
    "\n",
    "log_likelihood_x.eval({'x': observed_x})[i] = $\\log p(x_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate  \n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood result: tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "print('log likelihood result:', log_likelihood_x.eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we can easily calculate log likelihood by using pixyz.losses `LogProb()`.    \n",
    "The same calculation can be performed by defined probability distribution method `p.log_prob().eval()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogProb()\n",
      "tensor(-715.5875)\n",
      ".log_prob()\n",
      "tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "print('LogProb()')\n",
    "print(LogProb(p_nor_x).eval({'x': observed_x}).sum())\n",
    "print('.log_prob()')\n",
    "print(p_nor_x.log_prob().eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to probability density function:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the distance between probability distributions\n",
    "In the learning of generative models, we consider $p_{\\theta}(x)$ that is closed to the true distribution(data distribution) $p_{data}(x)$.  \n",
    "To find the appropriate parameter $\\theta$, we measure the distance between distributions.\n",
    "\n",
    "In VAE models we calculate Kullback-Leibler divergence, and in GAN models we calculate Jensen-Shannon divergence.  \n",
    "We can easily calculte the distance between distributions by Loss API  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://pixyz.readthedocs.io/en/latest/losses.html#adversarial-statistical-distance\n",
    "\n",
    "Here, we calculate the Kullback-Leibler divergence between a Gaussian distribution with mean=0, variance=1 and a Gaussian distribution with mean=5, variance=0.1  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$q(x) = \\cal N(\\mu=5, \\sigma^2=0.1)$  \n",
    "$KL(q(x) || p(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define probability distribution\n",
    "x_dim = 10\n",
    "# p \n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle q(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q\n",
    "q_nor_x = Normal(var=['x'], loc=torch.tensor(5.), scale=torch.tensor(0.1), features_shape=[x_dim], name='q')\n",
    "print_latex(q_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate Kullback-Leibler divergence, we use pixyz.losses `KullbackLeibler`.  \n",
    "We set the probability distribution defined in Pixyz distribution to `KullbackLeibler()`'s argument.  \n",
    "The value is calculated by `.eval()` method    \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#kullbackleibler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D_{KL} \\left[q(x)||p(x) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "kl_q_p = KullbackLeibler(q_nor_x, p_nor_x)\n",
    "print_latex(kl_q_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([143.0759])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculte the value\n",
    "kl_q_p.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to statistical distance:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://docs.pixyz.io/en/latest/losses.html#adversarial-statistical-distance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the expected value\n",
    "Expected value is a weighted average of all values of a random variable with a probability weight.\n",
    "In Pixyz, we calculate expected values of the variables which we can't feed as `input_var` like latent variable(Because it does't exist explicitly in the observation).  \n",
    "We can easily calculte the expected value of the random variables by Loss API.  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we consider these two probability distributions  \n",
    "$q(z|x) = \\cal N(\\mu=x, \\sigma^2=1)$  \n",
    "$p(x|z) = \\cal N(\\mu=z, \\sigma^2=1)$  \n",
    "and calculate following expected value  \n",
    "$\\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define probability distributions\n",
    "from pixyz.distributions import Normal\n",
    "\n",
    "q_nor_z__x = Normal(loc=\"x\", scale=torch.tensor(1.), var=[\"z\"], cond_var=[\"x\"],\n",
    "           features_shape=[10], name='q') # q(z|x)\n",
    "p_nor_x__z = Normal(loc=\"z\", scale=torch.tensor(1.), var=[\"x\"], cond_var=[\"z\"],\n",
    "                    features_shape=[10]) # p(x|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log p(x|z)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caltulate the Log likelihood of p(x|z)\n",
    "from pixyz.losses import LogProb\n",
    "\n",
    "p_log_likelihood = LogProb(p_nor_x__z)\n",
    "print_latex(p_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate expected values, we use pixyz.losses `Expectation`.  \n",
    "`Expectation()` has argument `p` and `f`.  \n",
    "We set the function of which we want to calculate expected values to argument `f`, and we set probability distributions which `f` function's random variable follows to argument `p`.  \n",
    "The value is calculated by `.eval()` method.  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import Expectation as E\n",
    "\n",
    "E_q_logprob_p = E(q_nor_z__x, LogProb(p_nor_x__z))\n",
    "print_latex(E_q_logprob_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.7006, -11.9861])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = torch.randn(2, 10)\n",
    "E_q_logprob_p.eval({'x': sample_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about Expectatoin API:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation considering data distribution(mean, sum) \n",
    "In theory, it is necessary to take the expected value of x, but since the data distribution is not actually given, we need to do calculations such as average and sum in the ovserved x batch direction.  \n",
    "We can easily calculte average and sum by Loss API.  \n",
    "Here, we calculate likelihood of training data observed_x and calculate its mean.  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$\\frac{1}{N} \\sum_{i=1}^N\\left[\\log p\\left(x^{(i)}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# observe x\n",
    "observed_x_num = 100\n",
    "x_dim = 5\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define probability distribution\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate sum or mean by `Loss.mean()` or `Loss.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(\\log p(x) \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# calculate mean\n",
    "mean_log_likelihood_x = LogProb(p_nor_x).mean() # .mean()\n",
    "print_latex(mean_log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.1973)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_log_likelihood_x.eval({'x': observed_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Loss\n",
    "We can do arithmetic operations between losses.  \n",
    "As an example, we define the following Loss by combining losses.  \n",
    "$\\frac{1}{N} \\sum_{i=1}^{N}\\left[\\mathbb{E}_{q\\left(z | x^{(i)}\\right)}\\left[\\log p\\left(x^{(i)} | z\\right)\\right]-K L\\left(q\\left(z | x^{(i)}\\right) \\| p(z)\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define probability distributions\n",
    "from pixyz.distributions import Normal\n",
    "\n",
    "# p(x|z)\n",
    "p_nor_x__z = Normal(loc=\"z\", scale=torch.tensor(1.), var=[\"x\"], cond_var=[\"z\"],\n",
    "                    features_shape=[10])\n",
    "\n",
    "# p(z)\n",
    "p_nor_z = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=[\"z\"],\n",
    "                    features_shape=[10])\n",
    "\n",
    "# q(z|x)\n",
    "q_nor_z__x = Normal(loc=\"x\", scale=torch.tensor(1.), var=[\"z\"], cond_var=[\"x\"],\n",
    "           features_shape=[10], name='q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(- D_{KL} \\left[q(z|x)||p(z) \\right] + \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define Loss\n",
    "from pixyz.losses import LogProb\n",
    "from pixyz.losses import Expectation as E\n",
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "# Log likelihood \n",
    "logprob_p_x__z = LogProb(p_nor_x__z)# input_var: x, z\n",
    "\n",
    "# Expecration\n",
    "E_q_z__x_logprob_p__z = E(q_nor_z__x, logprob_p_x__z)# input_car: x(z is not needed because of Expectation)\n",
    "\n",
    "# KL divergence\n",
    "KL_q_z__x_p_z = KullbackLeibler(q_nor_z__x, p_nor_z)\n",
    "\n",
    "# Subtraction between losses\n",
    "total_loss = E_q_z__x_logprob_p__z - KL_q_z__x_p_z# input_var: x(E_q_z__x_logprob_p__z needs x as input_var)\n",
    "\n",
    "# mean of loss\n",
    "total_loss = total_loss.mean()\n",
    "\n",
    "# check the loss\n",
    "print_latex(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.9965)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the loss value\n",
    "# observe x\n",
    "observed_x_num = 100\n",
    "x_dim = 10\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "\n",
    "# calculate the loss given observed x\n",
    "total_loss.eval({'x': observed_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we can define loss flexibly wth arithemtic operations between the Pixyz Loss API.  \n",
    "We can convert formulas to codes easily and intuitively with Pixyz Loss API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss API(ELBO)\n",
    "Pixyz Loss API has `ELBO` loss class.  \n",
    "\n",
    "Evidence Lower Bound ELBO: https://docs.pixyz.io/en/latest/losses.html#lower-bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Tutorial\n",
    "ModelAPITutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
