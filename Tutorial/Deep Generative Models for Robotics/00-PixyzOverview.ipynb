{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixyz API takes into account the features of deep generative models\n",
    "- The Deep Neural Network that composes the generative model is hidden by the probability distribution\n",
    "    - A framework that can separate defining DNNs and operating probability distributions(Distribution API)  \n",
    "- Model types and regularization of random variables are described as objective functions(error functions)\n",
    "    - A framework that receives probability distribution and define objective function(Loss API)  \n",
    "- Deep generative models learn by defining objective function and using gradient descent method\n",
    "    - A framework in which objective function and optimization algorithm can be set independently(Model API)\n",
    "<img src=\"../tutorial_figs/pixyz_API.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pixyz\n",
    "!pip install pixyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overviewing relationships between  each APIs through implementing VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Distribution API\n",
    "- A framework that can separate defining DNNs and operating probability distributions(Distribution API)\n",
    "- https://pixyz.readthedocs.io/en/latest/distributions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../tutorial_figs/vae_graphicalmodel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define these three probability distributions\n",
    "\n",
    "Prior: $p(z) = N(z; 0, 1)$\n",
    "\n",
    "Generator: $p_{\\theta}(x|z) = B(x; \\lambda = g(z))$\n",
    "\n",
    "Inference: $q_{\\phi}(z|x) = N(z; µ = f_{\\mu}(x), \\sigma^2 = f_{\\sigma^2}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define prior probability distribution\n",
    "\n",
    "prior is a gaussian distribution with mean 0 and variance 1\n",
    "\n",
    "$p(z) = N(z; 0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior\n",
    "z_dim = 64\n",
    "prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "              var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_latex(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define generator probability distribution\n",
    "Generator is a bernoulli distribution over x given z\n",
    "\n",
    "$p_{\\theta}(x|z) = B(x; \\lambda = g(z))$\n",
    "\n",
    "Inherit pixyz.Distribution class to define a distribution with Deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 784\n",
    "# generative model p(x|z)\n",
    "# inherit pixyz.Distribution Bernoulli class\n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "p = Generator().to(device)\n",
    "print(p)\n",
    "print_latex(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Inference probability distribution\n",
    "\n",
    "Inference is a gaussian distribution over z given x  \n",
    "$\\mu$ and $\\sigma$ are parameterized by $\\phi$\n",
    "\n",
    "$q_{\\phi}(z|x) = N(z; µ = f_{\\mu}(x), \\sigma^2 = f_{\\sigma^2}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference model q(z|x)\n",
    "# inherit pixyz.Distribution Normal class\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "q = Inference().to(device)\n",
    "print(q)\n",
    "print_latex(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from a probability distribution\n",
    "- Sampling can be done by .sample() in defined Distribution class regardless of DNN architecture or distribution type\n",
    "- In Pixyz, samples are dict type(key is variable name, value is sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z\\sim p(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z ~ p(z)\n",
    "prior_samples = prior.sample(batch_n=1)\n",
    "print(prior_samples)\n",
    "print(prior_samples.keys())\n",
    "print(prior_samples['z'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define joint distribution\n",
    "- joint distribution can be difined by multiplying distributions\n",
    "    - Sampling can be done by .sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{\\theta}(x, z) = p_{\\theta}(x|z)p(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_joint = p * prior\n",
    "print(p_joint)\n",
    "print_latex(p_joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from a joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x, z \\sim p_{\\theta}(x, z) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_joint_samples = p_joint.sample(batch_n=1)\n",
    "print(p_joint_samples)\n",
    "print(p_joint_samples.keys())\n",
    "print(p_joint_samples['x'].shape)\n",
    "print(p_joint_samples['z'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more detailed Distribution API Turorial\n",
    "- 01-DistributionAPITutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loss API\n",
    "- A framework that receives probability distribution and define objective function(Loss API)\n",
    "    - pixyz.Loss receives Distribution and defines Loss\n",
    "        - Arithmetic operations can be done between Loss classes, so any Loss can be designed\n",
    "            - -> Paper's formula can be put into codes easily\n",
    "- Loss value is evaluated by inputting the data\n",
    "    - Each Loss is treated as symbol\n",
    "        - Independent of data or DNN, we can design probabilistic model explicitly ->Define-and-run like framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE Loss\n",
    "$$\n",
    "-\\mathcal { L } _ { \\mathrm { VAE } } ( \\theta , \\phi ) =   \\mathbb { E } _ { p_{data}( x ) } \\left [D _ { \\mathrm { KL } } \\left[ q _ \\phi ( z | x ) \\| p ( z ) \\right] - \\mathbb { E } _ { q _ { \\phi } ( z | x ) } \\left[\\log p _ { \\theta } ( x | z ) \\right]\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss using pixyz.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = p.log_prob()\n",
    "print_latex(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.losses import Expectation as E\n",
    "reconst = E(q, log_p)\n",
    "print_latex(reconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.losses import KullbackLeibler\n",
    "kl = KullbackLeibler(q, prior)\n",
    "print_latex(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations between Loss classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_loss = (kl - reconst).mean()\n",
    "print_latex(vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data and loss is evaluated\n",
    "- loss is calculated by .eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_x for data\n",
    "dummy_x = torch.randn([4, 784]).to(device)\n",
    "vae_loss.eval({\"x\": dummy_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more detailed Loss API Turorial\n",
    "- 02-LossAPITutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model API\n",
    "- A framework in which objective function and optimization algorithm can be set independently\n",
    "- Set loss and optimization algorithm, then train with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.models import Model\n",
    "model = Model(loss=vae_loss, distributions=[p, q],\n",
    "             optimizer=optim.Adam, optimizer_params={\"lr\": 1e-3})\n",
    "print(model)\n",
    "print_latex(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = torch.randn([10, 784]).to(device)\n",
    "loss = model.train({\"x\": dummy_x})\n",
    "print('Train Loss: {:4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more detailed Model API Turorial\n",
    "- 03-ModelAPITutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training VAE with MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 20\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data'\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambd=lambda x: x.view(-1))])\n",
    "kwargs = {'batch_size': batch_size, 'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=True, transform=transform, download=True),\n",
    "    shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=False, transform=transform),\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Pixyz modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "from pixyz.losses import KullbackLeibler, Expectation as E\n",
    "from pixyz.models import Model\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 784\n",
    "z_dim = 64\n",
    "\n",
    "\n",
    "# inference model q(z|x)\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "    \n",
    "# generative model p(x|z)    \n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "    \n",
    "p = Generator().to(device)\n",
    "q = Inference().to(device)\n",
    "\n",
    "prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prior)\n",
    "print_latex(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)\n",
    "print_latex(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q)\n",
    "print_latex(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = KullbackLeibler(q, prior)\n",
    "reconst = -p.log_prob().expectation(q)\n",
    "vae_loss = (kl + reconst).mean()\n",
    "print_latex(vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set optimization algorithm and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(loss=vae_loss, distributions=[p, q],\n",
    "             optimizer=optim.Adam, optimizer_params={\"lr\": 1e-3})\n",
    "print(model)\n",
    "print_latex(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    #for x, _ in tqdm(train_loader):\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        loss = model.train({\"x\": x})\n",
    "        train_loss += loss\n",
    " \n",
    "    train_loss = train_loss * train_loader.batch_size / len(train_loader.dataset)\n",
    "    print('Epoch: {} Train loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss\n",
    "\n",
    "def test(epoch):\n",
    "    test_loss = 0\n",
    "    #for x, _ in tqdm(test_loader):\n",
    "    for x, _ in test_loader:\n",
    "        x = x.to(device)\n",
    "        loss = model.test({\"x\": x})\n",
    "        test_loss += loss\n",
    "\n",
    "    test_loss = test_loss * test_loader.batch_size / len(test_loader.dataset)\n",
    "    print('Test loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstrunction(x):\n",
    "    with torch.no_grad():\n",
    "        z = q.sample({\"x\": x}, return_all=False)\n",
    "        recon_batch = p.sample_mean(z).view(-1, 1, 28, 28)\n",
    "    \n",
    "        comparison = torch.cat([x.view(-1, 1, 28, 28), recon_batch]).cpu()\n",
    "        return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate images from latent variable space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_from_latent(z_sample):\n",
    "    with torch.no_grad():\n",
    "        sample = p.sample_mean({\"z\": z_sample}).view(-1, 1, 28, 28).cpu()\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_sample for generate imgs from prior\n",
    "z_sample = 0.5 * torch.randn(64, z_dim).to(device)\n",
    "\n",
    "# fixed _x for watching reconstruction improvement\n",
    "_x, _ = iter(test_loader).next()\n",
    "_x = _x.to(device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    \n",
    "    recon = plot_reconstrunction(_x[:8])\n",
    "    sample = plot_image_from_latent(z_sample)\n",
    "    \n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('Reconstruction')\n",
    "    imshow(torchvision.utils.make_grid(recon))\n",
    "    print('generate from prior z:')\n",
    "    imshow(torchvision.utils.make_grid(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
