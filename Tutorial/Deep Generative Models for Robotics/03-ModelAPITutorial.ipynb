{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep generative models learn by defining objective function and using gradient descent method\n",
    "- Unlike traditional generative models, deep generative models don't learn by sampling\n",
    "\n",
    "\n",
    "<img src='../tutorial_figs/pixyz_API.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A framework in which objective function and optimization algorithm can be set independently(Model API)\n",
    "- Model API document: https://docs.pixyz.io/en/v0.0.4/models.html  \n",
    "\n",
    "Here, we train the model with defined probability distributions and loss function by using Model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa0a01e6c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "batch_size = 256\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "root = '../data'\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambd=lambda x: x.view(-1))])\n",
    "kwargs = {'batch_size': batch_size, 'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=True, transform=transform, download=True),\n",
    "    shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=False, transform=transform),\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "\n",
    "x_dim = 784\n",
    "z_dim = 64\n",
    "\n",
    "# inference model q(z|x)\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "    \n",
    "# generative model p(x|z)    \n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "    \n",
    "gen_ber_x__z = Generator().to(device)\n",
    "infer_nor_z__x = Inference().to(device)\n",
    "\n",
    "prior_nor_z = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "from pixyz.losses import Expectation as E\n",
    "from pixyz.losses import KullbackLeibler\n",
    "from pixyz.utils import print_latex\n",
    "\n",
    "# log likelihood\n",
    "logprob_gen_x__z = LogProb(gen_ber_x__z)\n",
    "\n",
    "# Expectation\n",
    "E_infer_z__x_logprob_gen_x__z = E(infer_nor_z__x, logprob_gen_x__z)\n",
    "\n",
    "# KL divergence\n",
    "KL_infer_nor_z__x_prior_nor_z = KullbackLeibler(infer_nor_z__x, prior_nor_z)\n",
    "\n",
    "# Subtraction between losses\n",
    "total_loss = KL_infer_nor_z__x_prior_nor_z - E_infer_z__x_logprob_gen_x__z\n",
    "\n",
    "# mean of loss\n",
    "total_loss = total_loss.mean()\n",
    "\n",
    "\n",
    "# check the loss\n",
    "print_latex(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model API: Set probability distributions and loss, and optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pixyz.models Model.  \n",
    "Main arguments are `loss`, `distributions`, `optimizer`, `optimzer_params`. We set each arguments as follows.  \n",
    "- loss: Set defined loss function defined by Loss API\n",
    "- distributions: Set defined probability distributions which have parameters supposed to be learned defined by Distribution API  \n",
    "- optimizer, optimizer_params: Set optimization algorithms and parameters of the algorithm  \n",
    "\n",
    "For more details about Model: https://docs.pixyz.io/en/v0.0.4/_modules/pixyz/models/model.html#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.models import Model\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam\n",
    "optimizer_params = {'lr': 1e-3}\n",
    "\n",
    "vae_model = Model(loss=total_loss, \n",
    "                     distributions=[gen_ber_x__z, infer_nor_z__x],\n",
    "                     optimizer=optimizer,\n",
    "                     optimizer_params=optimizer_params\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined Model.  \n",
    "As shown above, we can set objective function and optimization algorithm independently.  \n",
    "Next, we train the model using `train()` method.  \n",
    "Model Class `train()` processes are following.  \n",
    "source code: https://docs.pixyz.io/en/v0.0.4/_modules/pixyz/models/model.html#Model.train\n",
    "1. Receive observed data x(.train({\"x\": x}))  \n",
    "2. Calculate loss  \n",
    "3. 1 step update of parameters  \n",
    "4. Return the loss value  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self, train_x={}, **kwargs):\n",
    "        self.distributions.train()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_cls.estimate(train_x, **kwargs)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # update params\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 199.86109924316406 \n",
      "Epoch 1, Loss 147.0438690185547 \n",
      "Epoch 2, Loss 126.67538452148438 \n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "for epoch in range(3):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        loss = vae_model.train({\"x\": x})\n",
    "        train_loss += loss\n",
    "    train_loss = train_loss * train_loader.batch_size / len(train_loader.dataset)\n",
    "    print('Epoch {}, Loss {} '.format(epoch, train_loss))\n",
    "    epoch_loss.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use more {abstract} Model API\n",
    "We can define models more easily by using more {abstract} Model API.  \n",
    "We need to set:  \n",
    "- define probability distributions  \n",
    "- (define additional loss functions)\n",
    "- select the optimization algorithm\n",
    "\n",
    "Here, we use VAE model as an example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "from pixyz.losses import KullbackLeibler\n",
    "# more {abstract} Model API VAE\n",
    "from pixyz.models import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 784\n",
    "z_dim = 64\n",
    "\n",
    "\n",
    "# inference model q(z|x)\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "    \n",
    "# generative model p(x|z)    \n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "    \n",
    "p = Generator().to(device)\n",
    "q = Inference().to(device)\n",
    "\n",
    "prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add regularization terms to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle D_{KL} \\left[q(z|x)||p_{prior}(z) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl = KullbackLeibler(q, prior)\n",
    "print_latex(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model: Set additional loss function and select the optimazation algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(encoder=q, decoder=p, regularizer=kl, \n",
    "            optimizer=optim.Adam, optimizer_params={\"lr\":1e-3})\n",
    "print_latex(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        loss = model.train({\"x\": x})\n",
    "        train_loss += loss\n",
    " \n",
    "    train_loss = train_loss * train_loader.batch_size / len(train_loader.dataset)\n",
    "    print('Epoch: {} Train loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 200.3801\n",
      "Epoch: 2 Train loss: 147.1353\n",
      "Epoch: 3 Train loss: 127.9876\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "train_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    train_losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more {abstract} Model\n",
    "- Pre-implementation models: https://docs.pixyz.io/en/v0.0.4/models.html#pre-implementation-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixyz implementations\n",
    "There are more complexed models written in pixyz in the following links.  \n",
    "- Pixyz examples: https://github.com/masa-su/pixyz/tree/master/examples\n",
    "- Pixyzoo: https://github.com/masa-su/pixyzoo\n",
    "\n",
    "Pixyz implementation work flow is the same for all models  \n",
    "1. Define probability distributions using `Distribution API`  \n",
    "1. Define the loss function based on the defined probability distributions using `Loss API`\n",
    "1. Set probability distributions and loss, and optimization algorithm using `Model API`, and train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
